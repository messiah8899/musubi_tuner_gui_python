import gradio as gr
import subprocess
import sys
import os
import signal
import psutil
from typing import Generator
import toml
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import re
import threading
import time
from collections import deque
from matplotlib import font_manager
import tempfile


def _setup_matplotlib_cjk_font():
    try:
        available_fonts = {f.name for f in font_manager.fontManager.ttflist}
        candidates = [
            'Microsoft YaHei',
            'Microsoft YaHei UI',
            'SimHei',
            'Noto Sans CJK SC',
            'Source Han Sans CN'
        ]
        chosen = None
        for name in candidates:
            if name in available_fonts:
                chosen = name
                break
        if chosen:
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [chosen]
        plt.rcParams['axes.unicode_minus'] = False
    except Exception:
        pass

_setup_matplotlib_cjk_font()


running_processes = {
    "cache": None,
    "train": None
}

# å…¨å±€å˜é‡å­˜å‚¨è®­ç»ƒæ•°æ®
training_data = {
    'steps': deque(maxlen=1000),
    'losses': deque(maxlen=1000),
    'timestamps': deque(maxlen=1000),
    'learning_rates': deque(maxlen=1000),
    'epochs': deque(maxlen=1000)
}

def parse_training_log(line):
    """è§£æè®­ç»ƒæ—¥å¿—ä¸­çš„losså’Œå…¶ä»–æŒ‡æ ‡"""
    # åŒ¹é…å¤šç§æ ¼å¼çš„è®­ç»ƒæ—¥å¿—
    patterns = [
        # æ ¼å¼1: steps: 0% | 20/10576 [00:32<4:23:26, 1.50s/it, avg_loss=0.113] æˆ– avr_loss=0.113
        r'steps:\s*(\d+)%\s*\|\s*(\d+)/(\d+)\s*\[.*?,\s*av[gr]_loss=([0-9.]+)\]',
        # æ ¼å¼2: | 10/10576 [00:18<5:32:12, 1.89s/it, avg_loss=0.112] æˆ– avr_loss=0.112
        r'\|\s*(\d+)/(\d+)\s*\[.*?,\s*av[gr]_loss=([0-9.]+)\]',
        # æ ¼å¼3: steps: 0%|10/10576[...]ï¼ˆæ— ç©ºæ ¼ä¹Ÿæ”¯æŒï¼‰
        r'steps:\s*(\d+)%\s*\|\s*(\d+)/(\d+)\s*\[.*?,\s*av[gr]_loss=([0-9.]+)\]'
    ]

    for i, pattern in enumerate(patterns):
        match = re.search(pattern, line)
        if match:
            if i == 0 or i == 2:  # æœ‰ç™¾åˆ†æ¯”çš„æ ¼å¼
                progress_percent = int(match.group(1))
                current_step = int(match.group(2))
                total_steps = int(match.group(3))
                avg_loss = float(match.group(4))
            else:  # æ²¡æœ‰ç™¾åˆ†æ¯”çš„æ ¼å¼
                current_step = int(match.group(1))
                total_steps = int(match.group(2))
                avg_loss = float(match.group(3))
                progress_percent = int((current_step / total_steps) * 100) if total_steps > 0 else 0

            # è®¡ç®—å½“å‰epochï¼ˆä¼°ç®—ï¼‰
            current_epoch = (current_step / total_steps) * 100 if total_steps > 0 else 0

            # æ·»åŠ åˆ°å…¨å±€æ•°æ®
            training_data['steps'].append(current_step)
            training_data['losses'].append(avg_loss)
            training_data['timestamps'].append(datetime.now())
            training_data['epochs'].append(current_epoch)
            training_data['learning_rates'].append(1e-5)  # é»˜è®¤å­¦ä¹ ç‡ï¼Œå¯ä»¥ä»å‚æ•°ä¸­è·å–



            return {
                'step': current_step,
                'total_steps': total_steps,
                'loss': avg_loss,
                'progress': progress_percent,
                'epoch': current_epoch
            }
    return None

project_root = Path(__file__).parent.absolute()
src_path = project_root / "src"

def create_loss_plot():
    """åˆ›å»ºLossæ›²çº¿å›¾"""
    plt.style.use('dark_background')
    fig, ax = plt.subplots(figsize=(10, 5))
    fig.patch.set_facecolor('#1a1a2e')
    ax.set_facecolor('#16213e')

    if len(training_data['steps']) > 0:
        steps = list(training_data['steps'])
        losses = list(training_data['losses'])

        # ç»˜åˆ¶lossæ›²çº¿
        ax.plot(steps, losses, color='#64b5f6', linewidth=2, alpha=0.9, label='Average Loss', marker='o', markersize=3)

        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(steps) > 5:
            import numpy as np
            z = np.polyfit(steps, losses, 1)
            p = np.poly1d(z)
            ax.plot(steps, p(steps), color='#ff7043', linestyle='--', alpha=0.7, label='Trend', linewidth=1.5)

        # æ˜¾ç¤ºæœ€æ–°çš„losså€¼
        if len(losses) > 0:
            latest_loss = losses[-1]
            latest_step = steps[-1]
            ax.annotate(f'Latest: {latest_loss:.4f}',
                       xy=(latest_step, latest_loss),
                       xytext=(10, 10), textcoords='offset points',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='#ff7043', alpha=0.7),
                       color='white', fontsize=10, fontweight='bold')

        # è®¾ç½®å›¾è¡¨æ ·å¼
        ax.set_xlabel('Training Steps', color='white', fontsize=11)
        ax.set_ylabel('Loss', color='white', fontsize=11)
        ax.set_title('å®æ—¶ Loss æ›²çº¿', color='white', fontsize=13, fontweight='bold')
        ax.grid(True, alpha=0.3, color='gray', linestyle=':')
        ax.legend(facecolor='#16213e', edgecolor='white', labelcolor='white', fontsize=10)

        # è®¾ç½®åæ ‡è½´é¢œè‰²
        ax.tick_params(colors='white', labelsize=9)
        for spine in ax.spines.values():
            spine.set_color('white')
    else:
        ax.text(0.5, 0.5, 'ç­‰å¾…è®­ç»ƒæ•°æ®...',
                horizontalalignment='center', verticalalignment='center',
                transform=ax.transAxes, color='white', fontsize=14)
        ax.set_title('å®æ—¶ Loss æ›²çº¿', color='white', fontsize=13, fontweight='bold')
        ax.set_xlabel('Training Steps', color='white', fontsize=11)
        ax.set_ylabel('Loss', color='white', fontsize=11)
        for spine in ax.spines.values():
            spine.set_color('white')

    plt.tight_layout()
    return fig

def create_progress_plot():
    """åˆ›å»ºè®­ç»ƒè¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯å›¾"""
    plt.style.use('dark_background')
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))
    fig.patch.set_facecolor('#1a1a2e')
    ax1.set_facecolor('#16213e')
    ax2.set_facecolor('#16213e')

    if len(training_data['steps']) > 0:
        steps = list(training_data['steps'])
        epochs = list(training_data['epochs'])
        losses = list(training_data['losses'])

        # ä¸Šå›¾ï¼šè®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”
        progress_percent = [(step / max(steps)) * 100 if max(steps) > 0 else 0 for step in steps]
        ax1.plot(steps, progress_percent, color='#4caf50', linewidth=2, alpha=0.9, label='è®­ç»ƒè¿›åº¦', marker='s', markersize=2)
        ax1.set_ylabel('è¿›åº¦ (%)', color='white', fontsize=11)
        ax1.set_title('è®­ç»ƒè¿›åº¦ & Loss ç»Ÿè®¡', color='white', fontsize=13, fontweight='bold')
        ax1.grid(True, alpha=0.3, color='gray', linestyle=':')
        ax1.legend(facecolor='#16213e', edgecolor='white', labelcolor='white', fontsize=10)
        ax1.tick_params(colors='white', labelsize=9)

        # æ˜¾ç¤ºå½“å‰è¿›åº¦
        if len(progress_percent) > 0:
            current_progress = progress_percent[-1]
            ax1.annotate(f'{current_progress:.1f}%',
                        xy=(steps[-1], current_progress),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='#4caf50', alpha=0.7),
                        color='white', fontsize=9, fontweight='bold')

        # ä¸‹å›¾ï¼šLosså˜åŒ–ç‡
        if len(losses) > 1:
            loss_changes = [0] + [losses[i] - losses[i-1] for i in range(1, len(losses))]
            colors = ['#ff5722' if change > 0 else '#2196f3' for change in loss_changes]
            ax2.bar(steps, loss_changes, color=colors, alpha=0.7, width=max(steps)*0.01 if max(steps) > 0 else 1)
            ax2.axhline(y=0, color='white', linestyle='-', alpha=0.5, linewidth=1)

        ax2.set_xlabel('Training Steps', color='white', fontsize=11)
        ax2.set_ylabel('Loss å˜åŒ–', color='white', fontsize=11)
        ax2.grid(True, alpha=0.3, color='gray', linestyle=':')
        ax2.tick_params(colors='white', labelsize=9)

        # è®¾ç½®åæ ‡è½´é¢œè‰²
        for ax in [ax1, ax2]:
            for spine in ax.spines.values():
                spine.set_color('white')
    else:
        ax1.text(0.5, 0.5, 'ç­‰å¾…è®­ç»ƒæ•°æ®...',
                horizontalalignment='center', verticalalignment='center',
                transform=ax1.transAxes, color='white', fontsize=14)
        ax1.set_title('è®­ç»ƒè¿›åº¦ & Loss ç»Ÿè®¡', color='white', fontsize=13, fontweight='bold')

        ax2.text(0.5, 0.5, 'ç­‰å¾…è®­ç»ƒæ•°æ®...',
                horizontalalignment='center', verticalalignment='center',
                transform=ax2.transAxes, color='white', fontsize=14)

        # è®¾ç½®åæ ‡è½´é¢œè‰²
        for ax in [ax1, ax2]:
            ax.set_xlabel('Training Steps', color='white', fontsize=11)
            ax.set_ylabel('', color='white', fontsize=11)
            for spine in ax.spines.values():
                spine.set_color('white')

    plt.tight_layout()
    return fig

def get_env_with_pythonpath():
    env = os.environ.copy()
    current_pythonpath = env.get("PYTHONPATH", "")
    if current_pythonpath:
        env["PYTHONPATH"] = f"{src_path}{os.pathsep}{current_pythonpath}"
    else:
        env["PYTHONPATH"] = str(src_path)
    return env

def terminate_process_tree(proc: subprocess.Popen):
    if proc is None:
        return
    try:
        parent_pid = proc.pid
        if parent_pid is None:
            return
        parent = psutil.Process(parent_pid)
        for child in parent.children(recursive=True):
            child.terminate()
        parent.terminate()
    except psutil.NoSuchProcess:
        pass
    except Exception as e:
        print(f"[WARN] terminate_process_tree error: {e}")

def stop_caching():
    if running_processes["cache"] is not None:
        proc = running_processes["cache"]
        if proc.poll() is None:
            terminate_process_tree(proc)
            running_processes["cache"] = None
            return "[INFO] ç¼“å­˜è¿›ç¨‹å·²åœæ­¢\n"
        else:
            return "[WARN] ç¼“å­˜è¿›ç¨‹å·²å®Œæˆ\n"
    else:
        return "[WARN] æ²¡æœ‰è¿è¡Œä¸­çš„ç¼“å­˜è¿›ç¨‹\n"

def stop_training():
    if running_processes["train"] is not None:
        proc = running_processes["train"]
        if proc.poll() is None:
            terminate_process_tree(proc)
            running_processes["train"] = None
            return "[INFO] è®­ç»ƒè¿›ç¨‹å·²åœæ­¢\n"
        else:
            return "[WARN] è®­ç»ƒè¿›ç¨‹å·²å®Œæˆ\n"
    else:
        return "[WARN] æ²¡æœ‰è¿è¡Œä¸­çš„è®­ç»ƒè¿›ç¨‹\n"

SETTINGS_FILE = "settings.toml"

def load_settings() -> dict:
    if os.path.exists(SETTINGS_FILE):
        try:
            with open(SETTINGS_FILE, "r", encoding="utf-8") as f:
                settings = toml.load(f)
                return settings
        except Exception:
            return {}
    else:
        return {}

def save_settings(settings: dict):
    try:
        with open(SETTINGS_FILE, "w", encoding="utf-8") as f:
            toml.dump(settings, f)
    except Exception as e:
        print(f"[WARN] Failed to save settings.toml: {e}")

def get_dataset_config(file_path: str, text_path: str) -> str:
    if file_path and os.path.isfile(file_path):
        return file_path
    elif text_path.strip():
        return text_path.strip()
    else:
        return ""

import platform
import shutil

def get_python_executable():
    """è·å–è·¨å¹³å°çš„Pythonå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        # åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ï¼Œä½¿ç”¨å½“å‰Python
        return sys.executable
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå…¥å¼Pythonï¼ˆWindowsï¼‰
    if platform.system() == "Windows":
        embedded_python = "./python_embeded/python.exe"
        if os.path.exists(embedded_python):
            return embedded_python
    
    # å°è¯•æŸ¥æ‰¾ç³»ç»ŸPython
    python_candidates = ["python3", "python"]
    for candidate in python_candidates:
        python_path = shutil.which(candidate)
        if python_path:
            return python_path
    
    # æœ€åä½¿ç”¨å½“å‰Python
    return sys.executable

python_executable = get_python_executable()

def run_hunyuan_cache(
    dataset_config_file: str,
    dataset_config_text: str,
    vae_path: str,
    text_encoder1_path: str,
    text_encoder2_path: str,
    enable_low_memory: bool,
    skip_existing: bool,
    use_clip: bool,
    clip_model_path: str
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings = {
        "hunyuan_cache": {
            "dataset_config_text": dataset_config_text,
            "vae_path": vae_path,
            "text_encoder1_path": text_encoder1_path,
            "text_encoder2_path": text_encoder2_path,
            "enable_low_memory": enable_low_memory,
            "skip_existing": skip_existing,
            "use_clip": use_clip,
            "clip_model_path": clip_model_path
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings)
    save_settings(existing_settings)

    cache_latents_cmd = [
        python_executable, "cache_latents.py",
        "--dataset_config", dataset_config,
        "--vae", vae_path,
        "--batch_size", "1"
    ]
    if enable_low_memory:
        cache_latents_cmd.extend(["--vae_spatial_tile_sample_min_size", "128", "--batch_size", "1"])
    if skip_existing:
        cache_latents_cmd.append("--skip_existing")
    if use_clip and clip_model_path.strip():
        cache_latents_cmd.extend(["--clip", clip_model_path.strip()])

    cache_text_encoder_cmd = [
        python_executable, "cache_text_encoder_outputs.py",
        "--dataset_config", dataset_config,
        "--text_encoder1", text_encoder1_path,
        "--text_encoder2", text_encoder2_path
    ]
    if enable_low_memory:
        cache_text_encoder_cmd.append("--fp8_llm")

    def run_and_stream_output(cmd):
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
        running_processes["cache"] = proc
        accumulated = ""
        for line in iter(proc.stdout.readline, ''):
            if line:
                accumulated += line
                yield accumulated
        proc.wait()
        running_processes["cache"] = None
        if proc.returncode != 0:
            accumulated += f"\n[ERROR] è¿›ç¨‹é€€å‡ºä»£ç  {proc.returncode}\n"
            yield accumulated

    accumulated_main = "\n[INFO] å¼€å§‹ HunyuanVideo æ½œåœ¨å˜é‡ç¼“å­˜ (cache_latents.py)...\n\n"
    yield accumulated_main
    for content in run_and_stream_output(cache_latents_cmd):
        yield content
    accumulated_main += "\n[INFO] HunyuanVideo æ½œåœ¨å˜é‡ç¼“å­˜å®Œæˆã€‚\n"
    yield accumulated_main

    accumulated_main += "\n[INFO] å¼€å§‹ HunyuanVideo æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜ (cache_text_encoder_outputs.py)...\n\n"
    yield accumulated_main
    for content in run_and_stream_output(cache_text_encoder_cmd):
        yield content
    accumulated_main += "\n[INFO] HunyuanVideo æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜å®Œæˆã€‚\n"
    yield accumulated_main

def run_hunyuan_training(
    dataset_config_file: str,
    dataset_config_text: str,
    dit_weights_path: str,
    max_train_epochs: int,
    learning_rate: str,
    network_dim: int,
    network_alpha: int,
    blocks_to_swap: int,
    output_dir: str,
    output_name: str,
    save_every_n_epochs: int,
    use_network_weights: bool,
    network_weights_path: str,
    gradient_checkpointing_cpu_offload: bool
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings = {
        "hunyuan_training": {
            "dataset_config_text": dataset_config_text,
            "dit_weights_path": dit_weights_path,
            "max_train_epochs": max_train_epochs,
            "learning_rate": learning_rate,
            "network_dim": network_dim,
            "network_alpha": network_alpha,
            "blocks_to_swap": blocks_to_swap,
            "output_dir": output_dir,
            "output_name": output_name,
            "save_every_n_epochs": save_every_n_epochs,
            "use_network_weights": use_network_weights,
            "network_weights_path": network_weights_path,
            "gradient_checkpointing_cpu_offload": gradient_checkpointing_cpu_offload
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings)
    save_settings(existing_settings)

    command = [
        python_executable, "-m", "accelerate.commands.launch",
        "--num_processes", "1",
        "--gpu_ids", "0",
        "--num_cpu_threads_per_process", "1",
        "--mixed_precision", "bf16",
        "hv_train_network.py",
        "--dit", dit_weights_path,
        "--dataset_config", dataset_config,
        "--sdpa",
        "--mixed_precision", "bf16",
        "--fp8_base",
        "--optimizer_type", "adamw8bit",
        "--learning_rate", learning_rate,
        "--gradient_checkpointing",
        "--max_data_loader_n_workers", "2",
        "--persistent_data_loader_workers",
        "--network_module=src.musubi_tuner.networks.lora",
        f"--network_dim={network_dim}",
        f"--network_alpha={network_alpha}",
        "--timestep_sampling", "sigmoid",
        "--discrete_flow_shift", "1.0",
        "--max_train_epochs", str(max_train_epochs),
        "--seed", "42",
        "--output_dir", output_dir,
        "--output_name", output_name,
        "--save_every_n_epochs", str(save_every_n_epochs),
        "--save_model_as", "safetensors",
        f"--blocks_to_swap={blocks_to_swap}"
    ]

    if gradient_checkpointing_cpu_offload:
        command.append("--gradient_checkpointing_cpu_offload")

    if use_network_weights and network_weights_path.strip():
        command.extend(["--network_weights", network_weights_path.strip()])

    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
    running_processes["train"] = proc
    accumulated = "[INFO] å¼€å§‹ HunyuanVideo LoRA è®­ç»ƒ...\n\n"
    yield accumulated
    last_yield = time.time()
    for line in iter(proc.stdout.readline, ''):
        if line:
            accumulated += line
            if time.time() - last_yield >= 1.5:
                last_yield = time.time()
                yield accumulated
    proc.wait()
    running_processes["train"] = None
    if proc.returncode != 0:
        accumulated += f"\n[ERROR] è®­ç»ƒé€€å‡ºä»£ç  {proc.returncode}\n"
    else:
        accumulated += "\n[INFO] è®­ç»ƒæˆåŠŸå®Œæˆï¼\n"
    yield accumulated

def run_qwen_cache(
    dataset_config_file: str,
    dataset_config_text: str,
    vae_path: str,
    dit_path: str,
    text_encoder_path: str,
    enable_low_memory: bool,
    skip_existing: bool
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings = {
        "qwen_cache": {
            "dataset_config_text": dataset_config_text,
            "vae_path": vae_path,
            "dit_path": dit_path,
            "text_encoder_path": text_encoder_path,
            "enable_low_memory": enable_low_memory,
            "skip_existing": skip_existing
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings)
    save_settings(existing_settings)

    cache_latents_cmd = [
        python_executable, "qwen_image_cache_latents.py",
        "--dataset_config", dataset_config,
        "--vae", vae_path,
        "--dit", dit_path
    ]
    if skip_existing:
        cache_latents_cmd.append("--skip_existing")

    cache_text_encoder_cmd = [
        python_executable, "qwen_image_cache_text_encoder_outputs.py",
        "--dataset_config", dataset_config,
        "--text_encoder", text_encoder_path
    ]
    if enable_low_memory:
        cache_text_encoder_cmd.append("--fp8_vl")

    def run_and_stream_output(cmd):
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=get_env_with_pythonpath())
        running_processes["cache"] = proc
        accumulated = ""
        for line in iter(proc.stdout.readline, ''):
            if line:
                accumulated += line
                yield accumulated
        proc.wait()
        running_processes["cache"] = None
        if proc.returncode != 0:
            accumulated += f"\n[ERROR] è¿›ç¨‹é€€å‡ºä»£ç  {proc.returncode}\n"
            yield accumulated

    accumulated_main = "\n[INFO] å¼€å§‹ Qwen-Image æ½œåœ¨å˜é‡ç¼“å­˜...\n\n"
    yield accumulated_main
    for content in run_and_stream_output(cache_latents_cmd):
        yield content
    accumulated_main += "\n[INFO] Qwen-Image æ½œåœ¨å˜é‡ç¼“å­˜å®Œæˆã€‚\n"
    yield accumulated_main

    accumulated_main += "\n[INFO] å¼€å§‹ Qwen-Image æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜...\n\n"
    yield accumulated_main
    for content in run_and_stream_output(cache_text_encoder_cmd):
        yield content
    accumulated_main += "\n[INFO] Qwen-Image æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜å®Œæˆã€‚\n"
    yield accumulated_main

def run_qwen_training(
    dataset_config_file: str,
    dataset_config_text: str,
    dit_weights_path: str,
    max_train_epochs: int,
    learning_rate: str,
    network_dim: int,
    network_alpha: int,
    output_dir: str,
    output_name: str,
    save_every_n_epochs: int,
    use_network_weights: bool,
    network_weights_path: str,
    enable_edit_mode: bool
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings = {
        "qwen_training": {
            "dataset_config_text": dataset_config_text,
            "dit_weights_path": dit_weights_path,
            "max_train_epochs": max_train_epochs,
            "learning_rate": learning_rate,
            "network_dim": network_dim,
            "network_alpha": network_alpha,
            "output_dir": output_dir,
            "output_name": output_name,
            "save_every_n_epochs": save_every_n_epochs,
            "use_network_weights": use_network_weights,
            "network_weights_path": network_weights_path,
            "enable_edit_mode": enable_edit_mode
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings)
    save_settings(existing_settings)

    command = [
        python_executable, "-m", "accelerate.commands.launch",
        "--num_processes", "1",
        "--gpu_ids", "0",
        "--num_cpu_threads_per_process", "1",
        "--mixed_precision", "bf16",
        "qwen_image_train_network.py",
        "--dit", dit_weights_path,
        "--dataset_config", dataset_config,
        "--sdpa",
        "--mixed_precision", "bf16",
        "--optimizer_type", "adamw8bit",
        "--learning_rate", learning_rate,
        "--gradient_checkpointing",
        "--max_data_loader_n_workers", "2",
        "--persistent_data_loader_workers",
        "--network_module=src.musubi_tuner.networks.lora_qwen_image",
        f"--network_dim={network_dim}",
        f"--network_alpha={network_alpha}",
        "--max_train_epochs", str(max_train_epochs),
        "--seed", "42",
        "--output_dir", output_dir,
        "--output_name", output_name,
        "--save_every_n_epochs", str(save_every_n_epochs),
        "--save_model_as", "safetensors"
    ]

    if enable_edit_mode:
        command.append("--edit")

    if use_network_weights and network_weights_path.strip():
        command.extend(["--network_weights", network_weights_path.strip()])

    # æ¸…ç©ºä¹‹å‰çš„è®­ç»ƒæ•°æ®
    training_data['steps'].clear()
    training_data['losses'].clear()
    training_data['timestamps'].clear()
    training_data['learning_rates'].clear()
    training_data['epochs'].clear()

    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=get_env_with_pythonpath())
    running_processes["train"] = proc
    accumulated = "[INFO] å¼€å§‹ Qwen-Image LoRA è®­ç»ƒ...\n\n"
    yield accumulated
    last_yield = time.time()
    for line in iter(proc.stdout.readline, ''):
        if line:
            accumulated += line
            parsed_data = parse_training_log(line)
            if parsed_data:
                if len(training_data['learning_rates']) > 0:
                    training_data['learning_rates'][-1] = float(learning_rate) if learning_rate else 1e-4
            if time.time() - last_yield >= 1.5:
                last_yield = time.time()
                yield accumulated
    proc.wait()
    running_processes["train"] = None
    if proc.returncode != 0:
        accumulated += f"\n[ERROR] è®­ç»ƒé€€å‡ºä»£ç  {proc.returncode}\n"
    else:
        accumulated += "\n[INFO] è®­ç»ƒæˆåŠŸå®Œæˆï¼\n"
    yield accumulated

def run_wan_cache(
    dataset_config_file: str,
    dataset_config_text: str,
    enable_low_memory: bool,
    skip_existing: bool,
    vae_path: str,
    t5_path: str,
    enable_i2v: bool,
    clip_model_path: str
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings_to_save = {
        "wan_cache": {
            "dataset_config_text": dataset_config_text,
            "enable_low_memory": enable_low_memory,
            "skip_existing": skip_existing,
            "vae_path": vae_path,
            "t5_path": t5_path,
            "enable_i2v": enable_i2v,
            "clip_path": clip_model_path
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings_to_save)
    save_settings(existing_settings)

    cache_latents_cmd = [
        python_executable, "wan_cache_latents.py",
        "--dataset_config", dataset_config,
        "--vae", vae_path
    ]
    if enable_low_memory:
        cache_latents_cmd.append("--vae_cache_cpu")
    if skip_existing:
        cache_latents_cmd.append("--skip_existing")
    if enable_i2v and clip_model_path and clip_model_path.strip():
        cache_latents_cmd.extend(["--clip", clip_model_path.strip()])

    cache_text_encoder_cmd = [
        python_executable, "wan_cache_text_encoder_outputs.py",
        "--dataset_config", dataset_config,
        "--t5", t5_path,
        "--batch_size", "16"
    ]
    if enable_low_memory:
        cache_text_encoder_cmd.append("--fp8_t5")
    if skip_existing:
        cache_text_encoder_cmd.append("--skip_existing")

    accumulated_main = "[INFO] å¼€å§‹ Wan æ½œåœ¨å˜é‡ç¼“å­˜ (wan_cache_latents.py)...\n\n"
    yield accumulated_main

    proc = subprocess.Popen(cache_latents_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=get_env_with_pythonpath())
    running_processes["cache"] = proc
    for line in iter(proc.stdout.readline, ''):
        if line:
            accumulated_main += line
            yield accumulated_main
    proc.wait()
    running_processes["cache"] = None
    if proc.returncode != 0:
        accumulated_main += f"\n[ERROR] æ½œåœ¨å˜é‡ç¼“å­˜é€€å‡ºä»£ç  {proc.returncode}\n"
        yield accumulated_main
        return

    accumulated_main += "\n[INFO] Wan æ½œåœ¨å˜é‡ç¼“å­˜å®Œæˆã€‚\n\n"
    yield accumulated_main

    accumulated_main += "[INFO] å¼€å§‹ Wan æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜ (wan_cache_text_encoder_outputs.py)...\n\n"
    yield accumulated_main

    proc = subprocess.Popen(cache_text_encoder_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=get_env_with_pythonpath())
    running_processes["cache"] = proc
    for line in iter(proc.stdout.readline, ''):
        if line:
            accumulated_main += line
            yield accumulated_main
    proc.wait()
    running_processes["cache"] = None
    if proc.returncode != 0:
        accumulated_main += f"\n[ERROR] æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜é€€å‡ºä»£ç  {proc.returncode}\n"
        yield accumulated_main
        return

    accumulated_main += "\n[INFO] Wan æ–‡æœ¬ç¼–ç å™¨ç¼“å­˜å®Œæˆã€‚\n"
    yield accumulated_main

def run_wan_training(
    task: str,
    dataset_config_file: str,
    dataset_config_text: str,
    dit_weights_path: str,
    is_wan22: bool,
    dit_low_noise_path: str,
    max_train_epochs: int,
    learning_rate: str,
    network_dim: int,
    network_alpha: int,
    blocks_to_swap: int,
    fp8: bool,
    output_dir: str,
    output_name: str,
    save_every_n_epochs: int,
    use_network_weights: bool,
    network_weights_path: str,
    attention_mode: str,
    mixed_precision: str,
    optimizer_type: str,
    gradient_accumulation_steps: int,
    max_grad_norm: float,
    lr_scheduler: str,
    lr_warmup_steps: int,
    timestep_sampling: str,
    discrete_flow_shift: float,
    weighting_scheme: str,
    enable_gradient_checkpointing: bool,
    seed: int,
    sample_every_n_epochs: int,
    sample_prompts: str,
    sample_steps: int,
    sample_solver: str,
    logging_dir: str,
    wandb_run_name: str
) -> Generator[str, None, None]:

    dataset_config = get_dataset_config(dataset_config_file, dataset_config_text)
    if not dataset_config:
        yield "[ERROR] è¯·æä¾›æ•°æ®é›†é…ç½®æ–‡ä»¶\n"
        return

    settings = {
        "wan_training": {
            "task": task,
            "dataset_config_text": dataset_config_text,
            "dit_weights_path": dit_weights_path,
            "is_wan22": is_wan22,
            "dit_low_noise_path": dit_low_noise_path,
            "max_train_epochs": max_train_epochs,
            "learning_rate": learning_rate,
            "network_dim": network_dim,
            "network_alpha": network_alpha,
            "blocks_to_swap": blocks_to_swap,
            "fp8": fp8,
            "output_dir": output_dir,
            "output_name": output_name,
            "save_every_n_epochs": save_every_n_epochs,
            "use_network_weights": use_network_weights,
            "network_weights_path": network_weights_path,
            "attention_mode": attention_mode,
            "mixed_precision": mixed_precision,
            "optimizer_type": optimizer_type,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "max_grad_norm": max_grad_norm,
            "lr_scheduler": lr_scheduler.split(" - ")[0] if " - " in lr_scheduler else lr_scheduler,
            "lr_warmup_steps": lr_warmup_steps,
            "timestep_sampling": timestep_sampling.split(" - ")[0] if " - " in timestep_sampling else timestep_sampling,
            "discrete_flow_shift": discrete_flow_shift,
            "weighting_scheme": weighting_scheme.split(" - ")[0] if " - " in weighting_scheme else weighting_scheme,
            "enable_gradient_checkpointing": enable_gradient_checkpointing,
            "seed": seed,
            "sample_every_n_epochs": sample_every_n_epochs,
            "sample_prompts": sample_prompts,
            "sample_steps": sample_steps,
            "sample_solver": sample_solver,
            "logging_dir": logging_dir,
            "wandb_run_name": wandb_run_name
        }
    }
    existing_settings = load_settings()
    existing_settings.update(settings)
    save_settings(existing_settings)

    command = [
        python_executable, "-m", "accelerate.commands.launch",
        "--num_processes", "1",
        "--gpu_ids", "0",
        "--num_cpu_threads_per_process", "1",
        "--mixed_precision", mixed_precision,
        "wan_train_network.py",
        "--task", task,
        "--dit", dit_weights_path,
        "--dataset_config", dataset_config,
        "--mixed_precision", mixed_precision,
        "--optimizer_type", optimizer_type,
        "--learning_rate", learning_rate,
        "--max_data_loader_n_workers", "2",
        "--persistent_data_loader_workers",
        "--network_module=src.musubi_tuner.networks.lora_wan",
        f"--network_dim={network_dim}",
        f"--network_alpha={network_alpha}",
        "--max_train_epochs", str(max_train_epochs),
        "--seed", str(seed),
        "--output_dir", output_dir,
        "--output_name", output_name,
        "--save_every_n_epochs", str(save_every_n_epochs),
        "--blocks_to_swap", str(blocks_to_swap),
        "--gradient_accumulation_steps", str(gradient_accumulation_steps),
        "--max_grad_norm", str(max_grad_norm),
        "--lr_scheduler", lr_scheduler.split(" - ")[0] if " - " in lr_scheduler else lr_scheduler,
        "--lr_warmup_steps", str(lr_warmup_steps),
        "--timestep_sampling", timestep_sampling.split(" - ")[0] if " - " in timestep_sampling else timestep_sampling,
        "--discrete_flow_shift", str(discrete_flow_shift),
        "--weighting_scheme", weighting_scheme.split(" - ")[0] if " - " in weighting_scheme else weighting_scheme
    ]

    if attention_mode == "sdpa":
        command.append("--sdpa")
    elif attention_mode == "flash_attn":
        command.append("--flash_attn")
    elif attention_mode == "sage_attn":
        command.append("--sage_attn")
    elif attention_mode == "xformers":
        command.append("--xformers")

    if enable_gradient_checkpointing:
        command.append("--gradient_checkpointing")

    if fp8:
        command.append("--fp8_base")

    if is_wan22 and dit_low_noise_path.strip():
        command.extend(["--dit_high_noise", dit_low_noise_path.strip()])

    if use_network_weights and network_weights_path.strip():
        command.extend(["--network_weights", network_weights_path.strip()])

    command.extend(["--sample_steps", str(sample_steps)])
    command.extend(["--sample_solver", sample_solver])

    if sample_every_n_epochs > 0 and sample_prompts.strip():
        temp_prompts_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
        temp_prompts_file.write(sample_prompts.strip())
        temp_prompts_file.close()

        command.extend(["--sample_every_n_epochs", str(sample_every_n_epochs)])
        command.extend(["--sample_prompts", temp_prompts_file.name])

    if logging_dir.strip():
        command.extend(["--logging_dir", logging_dir.strip()])
        command.append("--log_with=tensorboard")

    if wandb_run_name.strip():
        command.extend(["--wandb_run_name", wandb_run_name.strip()])
        command.append("--log_with=wandb")

    # æ¸…ç©ºä¹‹å‰çš„è®­ç»ƒæ•°æ®
    training_data['steps'].clear()
    training_data['losses'].clear()
    training_data['timestamps'].clear()
    training_data['learning_rates'].clear()
    training_data['epochs'].clear()

    accumulated = f"[INFO] å¼€å§‹ Wan LoRA è®­ç»ƒ (ä»»åŠ¡: {task})...\n"
    accumulated += f"[DEBUG] é‡‡æ ·æ­¥æ•°: {sample_steps}, é‡‡æ ·å™¨: {sample_solver}\n"
    accumulated += f"[DEBUG] å®Œæ•´å‘½ä»¤: {' '.join(command)}\n\n"
    yield accumulated

    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=get_env_with_pythonpath())
    running_processes["train"] = proc
    last_yield = time.time()
    for line in iter(proc.stdout.readline, ''):
        if line:
            accumulated += line
            parsed_data = parse_training_log(line)
            if parsed_data:
                if len(training_data['learning_rates']) > 0:
                    training_data['learning_rates'][-1] = float(learning_rate) if learning_rate else 1e-5
            if time.time() - last_yield >= 1.5:
                last_yield = time.time()
                yield accumulated
    proc.wait()
    running_processes["train"] = None
    if proc.returncode != 0:
        accumulated += f"\n[ERROR] è®­ç»ƒé€€å‡ºä»£ç  {proc.returncode}\n"
    else:
        accumulated += "\n[INFO] è®­ç»ƒæˆåŠŸå®Œæˆï¼\n"
    yield accumulated

settings = load_settings()



custom_css = """
.gradio-container {
    background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 50%, #e9ecef 100%) !important;
}
.tabs {
    background: rgba(0, 0, 0, 0.05) !important;
    border-radius: 12px !important;
    padding: 10px !important;
}
.tab-nav button {
    background: rgba(0, 0, 0, 0.08) !important;
    color: #2c3e50 !important;
    border: 1px solid rgba(0, 0, 0, 0.15) !important;
    font-weight: 600 !important;
    padding: 12px 24px !important;
    margin: 0 4px !important;
    border-radius: 8px 8px 0 0 !important;
}
.tab-nav button.selected {
    background: linear-gradient(135deg, #3498db 0%, #2980b9 100%) !important;
    border-bottom: 3px solid #2c3e50 !important;
    color: white !important;
}
label {
    color: #2c3e50 !important;
    font-weight: 600 !important;
    font-size: 14px !important;
}
.markdown {
    color: #2c3e50 !important;
}
h1 {
    color: #2980b9 !important;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.1) !important;
}
h2, h3 {
    color: #34495e !important;
}
.primary {
    background: linear-gradient(135deg, #3498db 0%, #2980b9 100%) !important;
    border: none !important;
    color: white !important;
    font-weight: 700 !important;
    box-shadow: 0 4px 6px rgba(0,0,0,0.2) !important;
}
.stop {
    background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%) !important;
    border: none !important;
    color: white !important;
    font-weight: 700 !important;
    box-shadow: 0 4px 6px rgba(0,0,0,0.2) !important;
}
input:not([type="checkbox"]):not([type="radio"]), textarea, select {
    background: rgba(255, 255, 255, 0.9) !important;
    border: 1px solid rgba(0, 0, 0, 0.2) !important;
    color: #2c3e50 !important;
    border-radius: 6px !important;
}
input[type="checkbox"], input[type="radio"] {
    accent-color: #3498db !important;
    cursor: pointer !important;
}
"""

with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title="Musubi Tuner v0.2.13") as demo:
    gr.Markdown("""
    # Musubi Tuner v0.2.13 ä¸“ä¸šè®­ç»ƒç•Œé¢
    ### ç•Œé¢ä½œè€…: suzuki & eddy | åŸºäº Kohya's Musubi Tuner
    **æ”¯æŒæ¨¡å‹:** HunyuanVideo | Wan2.1/2.2 | Qwen-Image | FramePack | FLUX Kontext
    ---
    """)

    with gr.Tab("å¿«é€Ÿå¼€å§‹"):
        gr.Markdown("""
        ## æ¬¢è¿ä½¿ç”¨ Musubi Tuner v0.2.13

        ### æ”¯æŒçš„æ¨¡å‹:
        - **HunyuanVideo**: æ–‡æœ¬è½¬è§†é¢‘ & å›¾åƒè½¬è§†é¢‘
        - **Wan2.1/2.2**: 1.3B & 14B æ¨¡å‹
        - **Qwen-Image**: æ–‡æœ¬è½¬å›¾åƒ & å›¾åƒç¼–è¾‘
        - **FramePack**: æ¸è¿›å¼å›¾åƒè½¬è§†é¢‘
        - **FLUX Kontext**: é«˜çº§å›¾åƒç”Ÿæˆ

        ### è®­ç»ƒæ­¥éª¤:
        1. **å‡†å¤‡æ•°æ®é›†** - åˆ›å»º TOML é…ç½®æ–‡ä»¶
        2. **é¢„ç¼“å­˜** - ç¼“å­˜æ½œåœ¨å˜é‡å’Œæ–‡æœ¬ç¼–ç å™¨è¾“å‡º
        3. **è®­ç»ƒ** - å¼€å§‹ LoRA è®­ç»ƒ
        4. **ç”Ÿæˆ** - ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA è¿›è¡Œæ¨ç†

        ### æ¨¡å‹ä¸‹è½½é“¾æ¥:
        - **HunyuanVideo**: [hunyuanvideo-community/HunyuanVideo](https://huggingface.co/hunyuanvideo-community/HunyuanVideo)
        - **Wan2.1**: [eddy1111111/WAN_train_models](https://huggingface.co/eddy1111111/WAN_train_models)
        - **Wan2.2**: [Comfy-Org/Wan_2.2_ComfyUI_Repackaged](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged)
        - **Qwen-Image**: [Comfy-Org/Qwen-Image_ComfyUI](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI)
        - **FramePack**: [lllyasviel/FramePackI2V_HY](https://huggingface.co/lllyasviel/FramePackI2V_HY)

        ### æ€§èƒ½ä¼˜åŒ–æç¤º:
        - **FP8 é‡åŒ–**: èŠ‚çœæ˜¾å­˜
        - **CPU å¸è½½**: èŠ‚çœ 20-30% æ˜¾å­˜
        - **ç½‘ç»œç»´åº¦**: ä» 16-32 å¼€å§‹ä»¥åŠ å¿«è®­ç»ƒ
        - **å—äº¤æ¢**: ä½¿ç”¨ 16-20 å—æ¥å‡å°‘æ˜¾å­˜ä½¿ç”¨
        - **ä½å†…å­˜æ¨¡å¼**: ä¸º 16GB ä»¥ä¸‹æ˜¾å­˜çš„ GPU å¯ç”¨

        ### æ¨¡å‹è·¯å¾„ç¤ºä¾‹:
        ```
        VAE: ./models/hunyuan/vae/diffusion_pytorch_model.safetensors
        Text Encoder 1: ./models/hunyuan/text_encoder/model-00001-of-00004.safetensors
        Text Encoder 2: ./models/hunyuan/text_encoder_2/model.safetensors
        DiT: ./models/hunyuan/hunyuan_video_fp8_scaled.safetensors
        ```
        """)
        gr.Markdown("""
        ### æ¨¡å‹æ”¾ç½®ä¸è·¯å¾„ä¹¦å†™è§„èŒƒï¼ˆè·¨å¹³å°ï¼‰
        - å»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œå¦‚: ./models/...
        - ä½¿ç”¨æ­£æ–œæ  /ï¼Œå…¼å®¹ Windows/Linux/macOS
        - è·¯å¾„å°½é‡é¿å…ä¸­æ–‡ä¸ç©ºæ ¼ï¼ˆåŒ…å«ç©ºæ ¼ä¹Ÿå¯ä½¿ç”¨ï¼‰
        - æ¨èç›®å½•ç»“æ„ï¼š
        ```
        ./models/
          qwen/
            qwen_image_bf16.safetensors
            qwen_image_vae.safetensors
            Qwen2.5-VL-7B-Instruct/
          wan/
            wan_2.1_vae.safetensors
            models_t5_umt5-xxl-enc-bf16.pth
            models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth
            wan_2.1_t2v_14b_bf16.safetensors
            wan_2.2_low_noise_bf16.safetensors
        ./datasets/
          my_dataset.toml
        ./output/
        ```
        """)


    with gr.Tab("Qwen-Image"):
        gr.Markdown("## Qwen-Image LoRA è®­ç»ƒ")
        with gr.Tabs():
            with gr.Tab("é¢„ç¼“å­˜"):
                gr.Markdown("""
                ### æ­¥éª¤ 1: ç¼“å­˜æ½œåœ¨å˜é‡å’Œæ–‡æœ¬ç¼–ç å™¨è¾“å‡º
                **æ‰€éœ€æ¨¡å‹:**
                - DiT: `qwen_image_bf16.safetensors` (å¿…é¡»ä½¿ç”¨ BF16 ç‰ˆæœ¬ï¼Œä¸èƒ½ä½¿ç”¨ FP8)
                - VAE: `qwen_image_vae.safetensors`
                - Text Encoder: Qwen2.5-VL æ¨¡å‹
                - ä¸‹è½½: [Comfy-Org/Qwen-Image_ComfyUI](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI)
                """)
                gr.Markdown("""
                #### è·¯å¾„ä¸ç›®å½•ç¤ºä¾‹ï¼ˆè·¨å¹³å°ï¼‰
                - VAE: ./models/qwen/qwen_image_vae.safetensors
                - DiT (BF16): ./models/qwen/qwen_image_bf16.safetensors
                - Text Encoder ç›®å½•: ./models/qwen/Qwen2.5-VL-7B-Instruct
                - æ•°æ®é›† TOML: ./datasets/my_dataset.toml
                - å»ºè®®ä½¿ç”¨æ­£æ–œæ  /ï¼Œé¿å…è½¬ä¹‰
                """)
                with gr.Row():
                    qw_cache_dataset_file = gr.File(label="ä¸Šä¼ æ•°æ®é›†é…ç½® (TOML)", file_count="single", file_types=[".toml"], type="filepath")
                    qw_cache_dataset_text = gr.Textbox(label="æˆ–è¾“å…¥ TOML è·¯å¾„", placeholder="./datasets/my_dataset.toml", value=settings.get("qwen_cache", {}).get("dataset_config_text", ""), interactive=True)
                qw_cache_low_memory = gr.Checkbox(label="å¯ç”¨ä½å†…å­˜æ¨¡å¼ (FP8 Text Encoder)", value=settings.get("qwen_cache", {}).get("enable_low_memory", False), interactive=True)
                qw_cache_skip_existing = gr.Checkbox(label="è·³è¿‡å·²å­˜åœ¨çš„ç¼“å­˜æ–‡ä»¶", value=settings.get("qwen_cache", {}).get("skip_existing", True), interactive=True)
                qw_cache_vae = gr.Textbox(label="VAE æ¨¡å‹è·¯å¾„", placeholder="./models/qwen/qwen_image_vae.safetensors", value=settings.get("qwen_cache", {}).get("vae_path", ""), interactive=True)
                qw_cache_dit = gr.Textbox(label="DiT æ¨¡å‹è·¯å¾„ (å¿…é¡» BF16)", placeholder="./models/qwen/qwen_image_bf16.safetensors", value=settings.get("qwen_cache", {}).get("dit_path", ""), interactive=True)
                qw_cache_te = gr.Textbox(label="Text Encoder è·¯å¾„ (Qwen2.5-VL)", placeholder="./models/qwen/Qwen2.5-VL-7B-Instruct", value=settings.get("qwen_cache", {}).get("text_encoder_path", ""), interactive=True)
                with gr.Row():
                    qw_cache_run_btn = gr.Button("å¼€å§‹é¢„ç¼“å­˜", variant="primary", size="lg")
                    qw_cache_stop_btn = gr.Button("åœæ­¢", variant="stop", size="lg")
                qw_cache_output = gr.Textbox(label="ç¼“å­˜è¾“å‡ºæ—¥å¿—", lines=20, interactive=False, show_copy_button=True)
                qw_cache_run_btn.click(fn=run_qwen_cache, inputs=[qw_cache_dataset_file, qw_cache_dataset_text, qw_cache_vae, qw_cache_dit, qw_cache_te, qw_cache_low_memory, qw_cache_skip_existing], outputs=qw_cache_output)
                qw_cache_stop_btn.click(fn=stop_caching, outputs=qw_cache_output)

            with gr.Tab("è®­ç»ƒ"):
                gr.Markdown("""
                ### æ­¥éª¤ 2: è®­ç»ƒ LoRA æ¨¡å‹
                **æ‰€éœ€æ¨¡å‹:**
                - DiT: `qwen_image_bf16.safetensors` (å¿…é¡»ä½¿ç”¨ BF16 ç‰ˆæœ¬)
                - ä¸‹è½½: [Comfy-Org/Qwen-Image_ComfyUI](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI)

                **æ¨¡å¼è¯´æ˜:**
                - **æ–‡æœ¬è½¬å›¾åƒ (T2I)**: æ ‡å‡†å›¾åƒç”Ÿæˆè®­ç»ƒ
                - **å›¾åƒç¼–è¾‘æ¨¡å¼**: å¯ç”¨åæ”¯æŒå›¾åƒç¼–è¾‘åŠŸèƒ½ (Edit-2509)
                """)
                gr.Markdown("""
                #### è·¯å¾„ä¸ç›®å½•ç¤ºä¾‹ï¼ˆè·¨å¹³å°ï¼‰
                - DiT (BF16): ./models/qwen/qwen_image_bf16.safetensors
                - è¾“å‡ºç›®å½•: ./output
                - ç»§ç»­è®­ç»ƒçš„ LoRA: ./output/qwen_lora.safetensors
                - æ•°æ®é›† TOML: ./datasets/my_dataset.toml
                - å»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„å’Œæ­£æ–œæ  /ï¼Œå…¼å®¹ Windows/Linux/macOS
                """)
                with gr.Row():
                    qw_train_dataset_file = gr.File(label="ä¸Šä¼ æ•°æ®é›†é…ç½® (TOML)", file_count="single", file_types=[".toml"], type="filepath")
                    qw_train_dataset_text = gr.Textbox(label="æˆ–è¾“å…¥ TOML è·¯å¾„", placeholder="./datasets/my_dataset.toml", value=settings.get("qwen_training", {}).get("dataset_config_text", ""), interactive=True)
                qw_train_dit = gr.Textbox(label="DiT æ¨¡å‹è·¯å¾„ (å¿…é¡» BF16)", placeholder="./models/qwen/qwen_image_bf16.safetensors", value=settings.get("qwen_training", {}).get("dit_weights_path", ""), interactive=True)
                with gr.Row():
                    qw_train_epochs = gr.Number(label="è®­ç»ƒè½®æ•°", value=settings.get("qwen_training", {}).get("max_train_epochs", 16), precision=0, minimum=1, interactive=True)
                    qw_train_lr = gr.Textbox(label="å­¦ä¹ ç‡", value=settings.get("qwen_training", {}).get("learning_rate", "1e-4"), placeholder="1e-4", interactive=True)
                with gr.Row():
                    qw_train_network_dim = gr.Number(label="ç½‘ç»œç»´åº¦ (LoRA rank)", value=settings.get("qwen_training", {}).get("network_dim", 32), precision=0, minimum=1, interactive=True)
                    qw_train_network_alpha = gr.Number(label="ç½‘ç»œ Alpha", value=settings.get("qwen_training", {}).get("network_alpha", 16), precision=0, minimum=1, interactive=True)
                qw_train_edit_mode = gr.Checkbox(label="å¯ç”¨å›¾åƒç¼–è¾‘æ¨¡å¼ (Edit-2509)", value=settings.get("qwen_training", {}).get("enable_edit_mode", False), interactive=True)
                with gr.Row():
                    qw_train_output_dir = gr.Textbox(label="è¾“å‡ºç›®å½•", value=settings.get("qwen_training", {}).get("output_dir", "./output"), placeholder="./output", interactive=True)
                    qw_train_output_name = gr.Textbox(label="è¾“å‡ºåç§°", value=settings.get("qwen_training", {}).get("output_name", "qwen_lora"), placeholder="qwen_lora", interactive=True)
                qw_train_save_every = gr.Number(label="æ¯ N è½®ä¿å­˜ä¸€æ¬¡", value=settings.get("qwen_training", {}).get("save_every_n_epochs", 2), precision=0, minimum=1, interactive=True)
                with gr.Row():
                    qw_train_use_network_weights = gr.Checkbox(label="ç»§ç»­è®­ç»ƒ (åŠ è½½å·²æœ‰ LoRA)", value=settings.get("qwen_training", {}).get("use_network_weights", False), interactive=True)
                    qw_train_network_weights_path = gr.Textbox(label="å·²æœ‰ LoRA è·¯å¾„", placeholder="./output/qwen_lora.safetensors", visible=settings.get("qwen_training", {}).get("use_network_weights", False), value=settings.get("qwen_training", {}).get("network_weights_path", ""), interactive=True)
                qw_train_use_network_weights.change(lambda x: gr.update(visible=x), inputs=qw_train_use_network_weights, outputs=qw_train_network_weights_path)
                with gr.Row():
                    qw_train_run_btn = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
                    qw_train_stop_btn = gr.Button("åœæ­¢", variant="stop", size="lg")
                qw_train_output = gr.Textbox(label="è®­ç»ƒè¾“å‡ºæ—¥å¿—", lines=20, interactive=False, show_copy_button=True)

                # æ·»åŠ å®æ—¶è®­ç»ƒæ›²çº¿å›¾
                gr.Markdown("### ğŸ“Š å®æ—¶è®­ç»ƒç›‘æ§")
                with gr.Row():
                    with gr.Column():
                        qw_loss_plot = gr.Plot(label="Loss æ›²çº¿", value=create_loss_plot())
                    with gr.Column():
                        qw_progress_plot = gr.Plot(label="è®­ç»ƒè¿›åº¦ & å­¦ä¹ ç‡", value=create_progress_plot())

                # åˆ›å»ºå®šæ—¶æ›´æ–°å‡½æ•°
                def update_qw_plots():
                    return create_loss_plot(), create_progress_plot()

                # è®¾ç½®å®šæ—¶å™¨æ›´æ–°å›¾è¡¨ï¼ˆæ¯2ç§’æ›´æ–°ä¸€æ¬¡ï¼Œæ›´å®æ—¶ï¼‰
                qw_plot_timer = gr.Timer(2)
                qw_plot_timer.tick(fn=update_qw_plots, outputs=[qw_loss_plot, qw_progress_plot])

                qw_train_run_btn.click(fn=run_qwen_training, inputs=[qw_train_dataset_file, qw_train_dataset_text, qw_train_dit, qw_train_epochs, qw_train_lr, qw_train_network_dim, qw_train_network_alpha, qw_train_output_dir, qw_train_output_name, qw_train_save_every, qw_train_use_network_weights, qw_train_network_weights_path, qw_train_edit_mode], outputs=qw_train_output)
                qw_train_stop_btn.click(fn=stop_training, outputs=qw_train_output)

    with gr.Tab("Wan2.1/2.2"):
        gr.Markdown("## Wan2.1/2.2 LoRA è®­ç»ƒ")
        gr.Markdown("""
        **æ³¨æ„**: Wan2.1 å’Œ Wan2.2 ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæµç¨‹
        - Wan2.1: æ”¯æŒ T2V å’Œ I2V
        - Wan2.2: ä»…æ”¯æŒ 14B æ¨¡å‹ï¼Œä½¿ç”¨åŒ DiT æ¶æ„ (é«˜å™ªå£° + ä½å™ªå£°)
        """)
        with gr.Tabs():
            with gr.Tab("é¢„ç¼“å­˜"):
                gr.Markdown("""
                ### æ­¥éª¤ 1: ç¼“å­˜æ½œåœ¨å˜é‡å’Œæ–‡æœ¬ç¼–ç å™¨è¾“å‡º
                **æ‰€éœ€æ¨¡å‹:**
                - VAE: `wan_2.1_vae.safetensors` æˆ– `Wan2.1_VAE.pth`
                - T5: `models_t5_umt5-xxl-enc-bf16.pth`
                - CLIP: `models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth` (ä»… Wan2.1 éœ€è¦)
                - ä¸‹è½½: [eddy1111111/WAN_train_models](https://huggingface.co/eddy1111111/WAN_train_models)
                """)
                gr.Markdown("""
                ####






                ####
                ####
                ####
                ####
""")
                gr.Markdown("""
                #### è·¯å¾„ä¸ç›®å½•ç¤ºä¾‹ï¼ˆè·¨å¹³å°ï¼‰
                - VAE: ./models/wan/wan_2.1_vae.safetensors
                - T5: ./models/wan/models_t5_umt5-xxl-enc-bf16.pth
                - æ•°æ®é›† TOML: ./datasets/my_dataset.toml
                - å»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„å’Œæ­£æ–œæ  /ï¼Œå…¼å®¹ Windows/Linux/macOS

                **æ³¨æ„**: å¤§éƒ¨åˆ†ç”¨æˆ·ä½¿ç”¨ T2V æ¨¡å¼ï¼Œä¸éœ€è¦å¯ç”¨ I2V é€‰é¡¹
                """)
                with gr.Row():
                    wan_cache_dataset_file = gr.File(label="ä¸Šä¼ æ•°æ®é›†é…ç½® (TOML)", file_count="single", file_types=[".toml"], type="filepath")
                    wan_cache_dataset_text = gr.Textbox(label="æˆ–è¾“å…¥ TOML è·¯å¾„", placeholder="./datasets/my_dataset.toml", value=settings.get("wan_cache", {}).get("dataset_config_text", ""), interactive=True)
                wan_cache_low_memory = gr.Checkbox(label="å¯ç”¨ä½å†…å­˜æ¨¡å¼ (FP8 T5)", value=settings.get("wan_cache", {}).get("enable_low_memory", False), interactive=True)
                wan_cache_skip_existing = gr.Checkbox(label="è·³è¿‡å·²å­˜åœ¨çš„ç¼“å­˜æ–‡ä»¶", value=settings.get("wan_cache", {}).get("skip_existing", True), interactive=True)
                wan_cache_vae = gr.Textbox(label="VAE æ¨¡å‹è·¯å¾„", placeholder="./models/wan/wan_2.1_vae.safetensors", value=settings.get("wan_cache", {}).get("vae_path", ""), interactive=True)
                wan_cache_t5 = gr.Textbox(label="T5 æ¨¡å‹è·¯å¾„", placeholder="./models/wan/models_t5_umt5-xxl-enc-bf16.pth", value=settings.get("wan_cache", {}).get("t5_path", ""), interactive=True)
                wan_cache_enable_i2v = gr.Checkbox(label="å¯ç”¨ I2V æ¨¡å¼ (éœ€è¦ CLIP æ¨¡å‹ï¼Œä»… Wan2.1)", value=settings.get("wan_cache", {}).get("enable_i2v", False), interactive=True)
                wan_cache_clip = gr.Textbox(label="CLIP æ¨¡å‹è·¯å¾„", placeholder="./models/wan/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", value=settings.get("wan_cache", {}).get("clip_path", ""), visible=settings.get("wan_cache", {}).get("enable_i2v", False), interactive=True)
                wan_cache_enable_i2v.change(lambda x: gr.update(visible=x), inputs=wan_cache_enable_i2v, outputs=wan_cache_clip)
                with gr.Row():
                    wan_cache_run_btn = gr.Button("å¼€å§‹é¢„ç¼“å­˜", variant="primary", size="lg")
                    wan_cache_stop_btn = gr.Button("åœæ­¢", variant="stop", size="lg")
                wan_cache_output = gr.Textbox(label="ç¼“å­˜è¾“å‡ºæ—¥å¿—", lines=20, interactive=False, show_copy_button=True)
                wan_cache_run_btn.click(
                    fn=run_wan_cache,
                    inputs=[
                        wan_cache_dataset_file,
                        wan_cache_dataset_text,
                        wan_cache_low_memory,
                        wan_cache_skip_existing,
                        wan_cache_vae,
                        wan_cache_t5,
                        wan_cache_enable_i2v,
                        wan_cache_clip,
                    ],
                    outputs=wan_cache_output,
                )
                wan_cache_stop_btn.click(fn=stop_caching, outputs=wan_cache_output)

            with gr.Tab("è®­ç»ƒ"):
                gr.Markdown("""
                ### æ­¥éª¤ 2: è®­ç»ƒ LoRA æ¨¡å‹
                **æ‰€éœ€æ¨¡å‹:**
                - Wan2.1 DiT: ä» [eddy1111111/WAN_train_models](https://huggingface.co/eddy1111111/WAN_train_models) ä¸‹è½½
                - Wan2.2 DiT: ä» [Comfy-Org/Wan_2.2_ComfyUI_Repackaged](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged) ä¸‹è½½
                - æ”¯æŒ fp16, bf16, fp8_e4m3fn æ¨¡å‹ (ä¸æ”¯æŒ fp8_scaled)

                **ä»»åŠ¡æ¨¡å¼è¯´æ˜:**
                - **t2v-1.3B / t2v-14B**: Wan2.1 æ–‡æœ¬è½¬è§†é¢‘ (1.3B / 14B å‚æ•°)
                - **i2v-14B**: Wan2.1 å›¾åƒè½¬è§†é¢‘ (éœ€è¦ CLIP æ¨¡å‹)
                - **t2i-14B**: Wan2.1 æ–‡æœ¬è½¬å›¾åƒ
                - **t2v-1.3B-FC / t2v-14B-FC / i2v-14B-FC**: Wan2.1 Fun Control æ¨¡å‹
                - **t2v-A14B / i2v-A14B**: Wan2.2 åŒ DiT æ¨¡å‹ (éœ€è¦é«˜å™ªå£°å’Œä½å™ªå£°ä¸¤ä¸ª DiT)
                """)
                gr.Markdown("""
                #### è·¯å¾„ä¸ç›®å½•ç¤ºä¾‹ï¼ˆè·¨å¹³å°ï¼‰
                - Wan2.1 DiT: ./models/wan/wan_2.1_t2v_14b_bf16.safetensors
                - Wan2.2 ä½å™ªå£° DiT: ./models/wan/wan_2.2_low_noise_bf16.safetensors
                - è¾“å‡ºç›®å½•: ./output
                - ç»§ç»­è®­ç»ƒçš„ LoRA: ./output/wan_lora.safetensors
                - å»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„å’Œæ­£æ–œæ  /ï¼Œå…¼å®¹ Windows/Linux/macOS
                """)
                wan_train_task = gr.Dropdown(
                    label="ä»»åŠ¡æ¨¡å¼ (Task)",
                    choices=[
                        "t2v-1.3B", "t2v-14B", "i2v-14B", "t2i-14B", "flf2v-14B",
                        "t2v-1.3B-FC", "t2v-14B-FC", "i2v-14B-FC",
                        "t2v-A14B", "i2v-A14B"
                    ],
                    value=settings.get("wan_training", {}).get("task", "t2v-14B"),
                    info="Wan2.1: t2v/i2v/t2i/flf2v-1.3B/14B, FC=Fun Control | Wan2.2: t2v/i2v-A14B",
                    interactive=True
                )
                with gr.Row():
                    wan_train_dataset_file = gr.File(label="ä¸Šä¼ æ•°æ®é›†é…ç½® (TOML)", file_count="single", file_types=[".toml"], type="filepath")
                    wan_train_dataset_text = gr.Textbox(label="æˆ–è¾“å…¥ TOML è·¯å¾„", placeholder="./datasets/my_dataset.toml", value=settings.get("wan_training", {}).get("dataset_config_text", ""), interactive=True)
                wan_train_dit = gr.Textbox(label="DiT æ¨¡å‹è·¯å¾„", placeholder="./models/wan/wan_2.1_t2v_14b_bf16.safetensors", value=settings.get("wan_training", {}).get("dit_weights_path", ""), interactive=True)
                wan_train_is_wan22 = gr.Checkbox(label="ä½¿ç”¨ Wan2.2 (åŒ DiT æ¶æ„)", value=settings.get("wan_training", {}).get("is_wan22", False), interactive=True)
                wan_train_dit_low_noise = gr.Textbox(label="DiT ä½å™ªå£°æ¨¡å‹è·¯å¾„ (ä»… Wan2.2)", placeholder="./models/wan/wan_2.2_low_noise_bf16.safetensors", visible=settings.get("wan_training", {}).get("is_wan22", False), value=settings.get("wan_training", {}).get("dit_low_noise_path", ""), interactive=True)
                wan_train_is_wan22.change(lambda x: gr.update(visible=x), inputs=wan_train_is_wan22, outputs=wan_train_dit_low_noise)
                with gr.Row():
                    wan_train_epochs = gr.Number(label="è®­ç»ƒè½®æ•°", value=settings.get("wan_training", {}).get("max_train_epochs", 16), precision=0, minimum=1, interactive=True)
                    wan_train_lr = gr.Textbox(label="å­¦ä¹ ç‡", value=settings.get("wan_training", {}).get("learning_rate", "1e-5"), placeholder="1e-5", interactive=True)
                with gr.Row():
                    wan_train_network_dim = gr.Number(label="ç½‘ç»œç»´åº¦ (LoRA rank)", value=settings.get("wan_training", {}).get("network_dim", 32), precision=0, minimum=1, interactive=True)
                    wan_train_network_alpha = gr.Number(label="ç½‘ç»œ Alpha", value=settings.get("wan_training", {}).get("network_alpha", 16), precision=0, minimum=1, interactive=True)
                with gr.Row():
                    wan_train_blocks_to_swap = gr.Number(label="å—äº¤æ¢æ•°é‡ (0-36, æ¨è: 16)", value=settings.get("wan_training", {}).get("blocks_to_swap", 16), precision=0, minimum=0, maximum=36, interactive=True)
                    wan_train_fp8 = gr.Checkbox(label="å¯ç”¨ FP8 é‡åŒ–", value=settings.get("wan_training", {}).get("fp8", True), interactive=True)
                with gr.Row():
                    wan_train_output_dir = gr.Textbox(label="è¾“å‡ºç›®å½•", value=settings.get("wan_training", {}).get("output_dir", "./output"), placeholder="./output", interactive=True)
                    wan_train_output_name = gr.Textbox(label="è¾“å‡ºåç§°", value=settings.get("wan_training", {}).get("output_name", "wan_lora"), placeholder="wan_lora", interactive=True)
                wan_train_save_every = gr.Number(label="æ¯ N è½®ä¿å­˜ä¸€æ¬¡", value=settings.get("wan_training", {}).get("save_every_n_epochs", 2), precision=0, minimum=1, interactive=True)
                with gr.Row():
                    wan_train_use_network_weights = gr.Checkbox(label="ç»§ç»­è®­ç»ƒ (åŠ è½½å·²æœ‰ LoRA)", value=settings.get("wan_training", {}).get("use_network_weights", False), interactive=True)
                    wan_train_network_weights_path = gr.Textbox(label="å·²æœ‰ LoRA è·¯å¾„", placeholder="./output/wan_lora.safetensors", visible=settings.get("wan_training", {}).get("use_network_weights", False), value=settings.get("wan_training", {}).get("network_weights_path", ""), interactive=True)
                wan_train_use_network_weights.change(lambda x: gr.update(visible=x), inputs=wan_train_use_network_weights, outputs=wan_train_network_weights_path)
                with gr.Row():
                    wan_train_run_btn = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
                    wan_train_stop_btn = gr.Button("åœæ­¢", variant="stop", size="lg")
                wan_train_output = gr.Textbox(label="è®­ç»ƒè¾“å‡ºæ—¥å¿—", lines=20, interactive=False, show_copy_button=True)

            with gr.Tab("é«˜çº§è®­ç»ƒé€‰é¡¹"):
                gr.Markdown("""
                ### é«˜çº§è®­ç»ƒå‚æ•°é…ç½®
                **æ³¨æ„**: è¿™äº›å‚æ•°ä¼šå½±å“è®­ç»ƒè´¨é‡å’Œé€Ÿåº¦ï¼Œå»ºè®®æœ‰ç»éªŒçš„ç”¨æˆ·è°ƒæ•´
                """)

                gr.Markdown("#### æ³¨æ„åŠ›æœºåˆ¶")
                wan_adv_attention = gr.Dropdown(
                    label="æ³¨æ„åŠ›è®¡ç®—æ–¹å¼",
                    choices=["sdpa", "flash_attn", "sage_attn", "xformers"],
                    value=settings.get("wan_training", {}).get("attention_mode", "sage_attn"),
                    info="sdpa=PyTorchåŸç”Ÿ | flash_attn=FlashAttention | sage_attn=SageAttentionæ¨è | xformers=xFormers",
                    interactive=True
                )

                gr.Markdown("#### ç²¾åº¦ä¸ä¼˜åŒ–å™¨")
                with gr.Row():
                    wan_adv_mixed_precision = gr.Dropdown(
                        label="æ··åˆç²¾åº¦",
                        choices=["no", "fp16", "bf16"],
                        value=settings.get("wan_training", {}).get("mixed_precision", "bf16"),
                        info="bf16æ¨èç”¨äºRTX 30/40/50ç³»åˆ—",
                        interactive=True
                    )
                    wan_adv_optimizer = gr.Dropdown(
                        label="ä¼˜åŒ–å™¨ç±»å‹",
                        choices=["AdamW", "AdamW8bit", "AdaFactor", "Prodigy"],
                        value=settings.get("wan_training", {}).get("optimizer_type", "adamw8bit"),
                        info="AdamW8bitå¯èŠ‚çœæ˜¾å­˜",
                        interactive=True
                    )

                gr.Markdown("#### æ¢¯åº¦ä¸å­¦ä¹ ç‡")
                with gr.Row():
                    wan_adv_grad_accum = gr.Number(
                        label="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°",
                        value=settings.get("wan_training", {}).get("gradient_accumulation_steps", 1),
                        precision=0,
                        minimum=1,
                        info="å¢åŠ æ­¤å€¼å¯æ¨¡æ‹Ÿæ›´å¤§çš„batch size",
                        interactive=True
                    )
                    wan_adv_max_grad_norm = gr.Number(
                        label="æœ€å¤§æ¢¯åº¦èŒƒæ•°",
                        value=settings.get("wan_training", {}).get("max_grad_norm", 1.0),
                        minimum=0,
                        info="æ¢¯åº¦è£å‰ªï¼Œ0è¡¨ç¤ºä¸è£å‰ª",
                        interactive=True
                    )

                with gr.Row():
                    wan_adv_lr_scheduler = gr.Dropdown(
                        label="å­¦ä¹ ç‡è°ƒåº¦å™¨",
                        choices=[
                            "constant - æ’å®šå­¦ä¹ ç‡ï¼Œé€‚åˆå¾®è°ƒå’Œå°æ•°æ®é›†",
                            "constant_with_warmup - æ’å®š+é¢„çƒ­ï¼Œé€‚åˆå¤§æ¨¡å‹è®­ç»ƒ",
                            "cosine - ä½™å¼¦é€€ç«ï¼Œæ”¶æ•›å¹³æ»‘ï¼Œé€‚åˆé•¿æ—¶é—´è®­ç»ƒ",
                            "cosine_with_restarts - ä½™å¼¦+é‡å¯ï¼Œå‘¨æœŸæ€§é‡å¯ï¼Œé€‚åˆè·³å‡ºå±€éƒ¨æœ€ä¼˜",
                            "linear - çº¿æ€§è¡°å‡ï¼Œå­¦ä¹ ç‡çº¿æ€§ä¸‹é™åˆ°0",
                            "polynomial - å¤šé¡¹å¼è¡°å‡ï¼Œä»‹äºçº¿æ€§å’Œä½™å¼¦ä¹‹é—´"
                        ],
                        value=settings.get("wan_training", {}).get("lr_scheduler", "constant - æ’å®šå­¦ä¹ ç‡ï¼Œé€‚åˆå¾®è°ƒå’Œå°æ•°æ®é›†"),
                        info="å­¦ä¹ ç‡å˜åŒ–ç­–ç•¥",
                        interactive=True
                    )
                    wan_adv_lr_warmup = gr.Number(
                        label="é¢„çƒ­æ­¥æ•°",
                        value=settings.get("wan_training", {}).get("lr_warmup_steps", 0),
                        precision=0,
                        minimum=0,
                        info="å­¦ä¹ ç‡é€æ¸å¢åŠ çš„æ­¥æ•°",
                        interactive=True
                    )

                gr.Markdown("#### æ—¶é—´æ­¥é‡‡æ ·")
                with gr.Row():
                    wan_adv_timestep_sampling = gr.Dropdown(
                        label="æ—¶é—´æ­¥é‡‡æ ·æ–¹æ³•",
                        choices=[
                            "sigma - SD3é»˜è®¤ï¼Œå¹³è¡¡å„å™ªå£°çº§åˆ«",
                            "uniform - å‡åŒ€éšæœºï¼Œæ‰€æœ‰timestepæ¦‚ç‡ç›¸åŒ",
                            "sigmoid - sigmoidå˜æ¢ï¼Œæ›´å…³æ³¨ä¸­é—´å™ªå£°",
                            "shift - sigmoid+shiftï¼Œå¯è°ƒæ•´åˆ†å¸ƒ",
                            "flux_shift - FLUXä¼˜åŒ–ï¼Œé€‚åˆé«˜åˆ†è¾¨ç‡",
                            "qwen_shift - Qwenä¼˜åŒ–ç­–ç•¥",
                            "logsnr - åŸºäºlog-SNRï¼Œç†è®ºæ›´ä¼˜"
                        ],
                        value=settings.get("wan_training", {}).get("timestep_sampling", "sigma - SD3é»˜è®¤ï¼Œå¹³è¡¡å„å™ªå£°çº§åˆ«"),
                        info="å½±å“è®­ç»ƒæ—¶å™ªå£°åˆ†å¸ƒ",
                        interactive=True
                    )
                    wan_adv_flow_shift = gr.Number(
                        label="ç¦»æ•£æµåç§»",
                        value=settings.get("wan_training", {}).get("discrete_flow_shift", 1.0),
                        minimum=0.1,
                        maximum=10.0,
                        info="Eulerè°ƒåº¦å™¨çš„æµåç§»å‚æ•°",
                        interactive=True
                    )

                wan_adv_weighting = gr.Dropdown(
                    label="æƒé‡æ–¹æ¡ˆ",
                    choices=[
                        "none - æ— æƒé‡ï¼Œæ‰€æœ‰timestepæƒé‡ç›¸åŒ",
                        "logit_normal - logitæ­£æ€åˆ†å¸ƒï¼ŒSD3è®ºæ–‡æ¨è",
                        "mode - æ¨¡å¼æƒé‡",
                        "cosmap - ä½™å¼¦æ˜ å°„ï¼Œå¹³æ»‘è¿‡æ¸¡",
                        "sigma_sqrt - sigmaå¹³æ–¹æ ¹ï¼Œå¼ºè°ƒä½å™ªå£°"
                    ],
                    value=settings.get("wan_training", {}).get("weighting_scheme", "none - æ— æƒé‡ï¼Œæ‰€æœ‰timestepæƒé‡ç›¸åŒ"),
                    info="æ—¶é—´æ­¥åˆ†å¸ƒçš„æƒé‡ç­–ç•¥",
                    interactive=True
                )

                gr.Markdown("#### å†…å­˜ä¼˜åŒ–")
                wan_adv_gradient_checkpointing = gr.Checkbox(
                    label="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹",
                    value=settings.get("wan_training", {}).get("enable_gradient_checkpointing", True),
                    info="é™ä½æ˜¾å­˜ä½¿ç”¨ï¼Œä½†ä¼šç•¥å¾®é™ä½è®­ç»ƒé€Ÿåº¦",
                    interactive=True
                )

                gr.Markdown("#### è®­ç»ƒæ§åˆ¶")
                wan_adv_seed = gr.Number(
                    label="éšæœºç§å­",
                    value=settings.get("wan_training", {}).get("seed", 42),
                    precision=0,
                    info="è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯å¤ç°çš„ç»“æœ",
                    interactive=True
                )

                gr.Markdown("#### é‡‡æ ·ç”Ÿæˆ")

                with gr.Row():
                    wan_adv_sample_epochs = gr.Number(
                        label="æ¯Nè½®ç”Ÿæˆæ ·æœ¬",
                        value=settings.get("wan_training", {}).get("sample_every_n_epochs", 0),
                        precision=0,
                        minimum=0,
                        info="ä¾‹å¦‚ï¼š1=æ¯è½®éƒ½ç”Ÿæˆé¢„è§ˆï¼Œ5=æ¯5è½®ç”Ÿæˆä¸€æ¬¡ï¼Œ0=ç¦ç”¨é¢„è§ˆç”Ÿæˆ",
                        interactive=True
                    )
                    wan_adv_sample_steps = gr.Number(
                        label="é‡‡æ ·æ¨ç†æ­¥æ•°",
                        value=settings.get("wan_training", {}).get("sample_steps", 20),
                        precision=0,
                        minimum=1,
                        maximum=1000,
                        info="å…¨å±€é»˜è®¤æ¨ç†æ­¥æ•°ï¼Œè„šæœ¬é»˜è®¤20ï¼ŒWanæ¨è40ã€‚æç¤ºè¯æ–‡ä»¶ä¸­å¯ç”¨ --s è¦†ç›–æ­¤å€¼",
                        interactive=True
                    )

                with gr.Row():
                    wan_adv_sample_solver = gr.Dropdown(
                        label="é‡‡æ ·å™¨ç®—æ³•",
                        choices=["unipc", "dpm++", "vanilla", "sa_ode_stable"],
                        value=settings.get("wan_training", {}).get("sample_solver", "unipc"),
                        info="unipc=é»˜è®¤æ¨è | dpm++=DPMæ±‚è§£å™¨ | vanilla=åŸºç¡€ | sa_ode_stable=ç¨³å®šODEæ±‚è§£å™¨",
                        interactive=True
                    )

                wan_adv_sample_prompts = gr.Textbox(
                    label="æ ·æœ¬æç¤ºè¯",
                    value=settings.get("wan_training", {}).get("sample_prompts", ""),
                    placeholder="a beautiful sunset over the ocean\na cat playing with a ball --s 30",
                    lines=5,
                    info="ç›´æ¥è¾“å…¥æç¤ºè¯ï¼Œæ¯è¡Œä¸€ä¸ªã€‚å¯åœ¨è¡Œæœ«ä½¿ç”¨ --s è¦†ç›–å…¨å±€é‡‡æ ·æ­¥æ•°",
                    interactive=True
                )

                gr.Markdown("""
                **æç¤ºè¯æ ¼å¼ç¤ºä¾‹:**
                ```
                a woman walking --s 40 --w 640 --h 480 --f 16
                a cat playing
                a dog running in the park --s 40
                ```
                æ¯è¡Œä¸€ä¸ªæç¤ºè¯ï¼Œå¯é€‰å‚æ•°ï¼š--s=æ¨ç†æ­¥æ•° --w=å®½åº¦ --h=é«˜åº¦ --f=å¸§æ•° --d=ç§å­ --g=å¼•å¯¼æ¯”ä¾‹ --n=è´Ÿé¢æç¤ºè¯
                """)

                gr.Markdown("#### æ—¥å¿—ä¸ç›‘æ§")
                wan_adv_logging_dir = gr.Textbox(
                    label="TensorBoardæ—¥å¿—ç›®å½•",
                    value=settings.get("wan_training", {}).get("logging_dir", ""),
                    placeholder="./logs",
                    info="ç•™ç©ºåˆ™ä¸å¯ç”¨TensorBoard",
                    interactive=True
                )
                wan_adv_wandb_name = gr.Textbox(
                    label="WandBè¿è¡Œåç§°",
                    value=settings.get("wan_training", {}).get("wandb_run_name", ""),
                    placeholder="wan_training_run",
                    info="ç•™ç©ºåˆ™ä¸å¯ç”¨WandB",
                    interactive=True
                )

                # æ·»åŠ å®æ—¶è®­ç»ƒæ›²çº¿å›¾
                gr.Markdown("### ğŸ“Š å®æ—¶è®­ç»ƒç›‘æ§")
                with gr.Row():
                    with gr.Column():
                        wan_loss_plot = gr.Plot(label="Loss æ›²çº¿", value=create_loss_plot())
                    with gr.Column():
                        wan_progress_plot = gr.Plot(label="è®­ç»ƒè¿›åº¦ & å­¦ä¹ ç‡", value=create_progress_plot())

                # åˆ›å»ºå®šæ—¶æ›´æ–°å‡½æ•°
                def update_plots():
                    return create_loss_plot(), create_progress_plot()

                # è®¾ç½®å®šæ—¶å™¨æ›´æ–°å›¾è¡¨ï¼ˆæ¯2ç§’æ›´æ–°ä¸€æ¬¡ï¼Œæ›´å®æ—¶ï¼‰
                wan_plot_timer = gr.Timer(2)
                wan_plot_timer.tick(fn=update_plots, outputs=[wan_loss_plot, wan_progress_plot])

                wan_train_run_btn.click(
                    fn=run_wan_training,
                    inputs=[
                        wan_train_task,
                        wan_train_dataset_file,
                        wan_train_dataset_text,
                        wan_train_dit,
                        wan_train_is_wan22,
                        wan_train_dit_low_noise,
                        wan_train_epochs,
                        wan_train_lr,
                        wan_train_network_dim,
                        wan_train_network_alpha,
                        wan_train_blocks_to_swap,
                        wan_train_fp8,
                        wan_train_output_dir,
                        wan_train_output_name,
                        wan_train_save_every,
                        wan_train_use_network_weights,
                        wan_train_network_weights_path,
                        wan_adv_attention,
                        wan_adv_mixed_precision,
                        wan_adv_optimizer,
                        wan_adv_grad_accum,
                        wan_adv_max_grad_norm,
                        wan_adv_lr_scheduler,
                        wan_adv_lr_warmup,
                        wan_adv_timestep_sampling,
                        wan_adv_flow_shift,
                        wan_adv_weighting,
                        wan_adv_gradient_checkpointing,
                        wan_adv_seed,
                        wan_adv_sample_epochs,
                        wan_adv_sample_prompts,
                        wan_adv_sample_steps,
                        wan_adv_sample_solver,
                        wan_adv_logging_dir,
                        wan_adv_wandb_name
                    ],
                    outputs=wan_train_output
                )
                wan_train_stop_btn.click(fn=stop_training, outputs=wan_train_output)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

